# High-Dimensional Analysis of Double Descent  
### Based on Francis Bach (2023)

## Overview
This project studies the double descent phenomenon in linear regression and provides a high-dimensional analysis following Francis Bach (2023).

The report explains:

- Classical double descent in linear regression
- Ridge regression in high dimensions
- Self-induced regularization κ(λ)
- Random matrix theory interpretation
- Random projections and asymptotic risk
- Connection to neural networks
- Unification with grokking

## Key Contributions

- Mathematical explanation of implicit regularization
- Implementation of random projection experiments
- Empirical validation of bias-variance decomposition
- Connection between interpolation and SGD dynamics

## Structure

- `report/` → Final PDF report
- `experiments/` → Simulation code
- `figures/` → Generated plots

## Main References

- Bach (2023)
- Belkin et al. (2019, 2020)
- Davies et al. (2023)

---

Author: Sahibnoor Singh  
LMU Munich — MSc Statistics & Data Science
